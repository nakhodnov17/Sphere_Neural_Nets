{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 50\n",
      "\tCRITIC_ITERS: 5\n",
      "\tD: Variable containing:\n",
      " 3.2323\n",
      " 1.6293\n",
      " 2.6910\n",
      " 1.2733\n",
      "-0.1109\n",
      " 1.2385\n",
      " 1.9894\n",
      " 1.5320\n",
      " 2.5893\n",
      " 1.1832\n",
      " 2.3117\n",
      " 1.9274\n",
      " 0.1745\n",
      " 2.9381\n",
      "-0.1292\n",
      " 1.6020\n",
      " 0.6219\n",
      "-0.1378\n",
      " 2.0453\n",
      "-0.0771\n",
      " 4.5096\n",
      " 0.9310\n",
      " 0.3904\n",
      " 0.1515\n",
      " 0.4336\n",
      " 1.3959\n",
      " 3.1922\n",
      " 1.6378\n",
      " 1.9011\n",
      " 1.8315\n",
      " 2.9631\n",
      " 3.0266\n",
      " 1.1532\n",
      "-0.3384\n",
      " 1.0455\n",
      " 1.1279\n",
      "-0.0669\n",
      " 1.6637\n",
      " 3.1611\n",
      " 1.6208\n",
      " 2.3204\n",
      " 3.1992\n",
      "-1.0415\n",
      " 2.3218\n",
      " 0.2718\n",
      " 1.4206\n",
      " 1.8987\n",
      " 1.5400\n",
      " 2.2228\n",
      " 2.6847\n",
      "[torch.cuda.FloatTensor of size 50 (GPU 0)]\n",
      "\n",
      "\tDIM: 64\n",
      "\tF: <module 'torch.nn.functional' from '/home/m.nakhodnov/anaconda3/envs/py2.7.14/lib/python2.7/site-packages/torch/nn/functional.pyc'>\n",
      "\tG: Variable containing:\n",
      "-0.2916\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "\tITERS: 200000\n",
      "\tLAMBDA: 10\n",
      "\tOUTPUT_DIM: 784\n",
      "Generator(\n",
      "  (block1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      "  (deconv_out): ConvTranspose2d(64, 1, kernel_size=(8, 8), stride=(2, 2))\n",
      "  (preprocess): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (5): ReLU(inplace)\n",
      "  )\n",
      "  (output): Linear(in_features=4096, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "iter 8017\ttmp/mnist/wasserstein distance\t2.08622169495\ttmp/mnist/train gen cost\t0.783775925636\ttmp/mnist/time\t0.174247595999\ttmp/mnist/train disc cost\t-1.35502779484\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "iter 8018\ttmp/mnist/wasserstein distance\t0.670475006104\ttmp/mnist/train gen cost\t0.688974022865\ttmp/mnist/time\t0.172699928284\ttmp/mnist/train disc cost\t6.89382410049\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "iter 8019\ttmp/mnist/wasserstein distance\t1.85497701168\ttmp/mnist/train gen cost\t1.72891247272\ttmp/mnist/time\t0.161427974701\ttmp/mnist/train disc cost\t3.10531711578\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "iter 8020\ttmp/mnist/wasserstein distance\t4.30185651779\ttmp/mnist/train gen cost\t3.70304107666\ttmp/mnist/time\t0.158775091171\ttmp/mnist/train disc cost\t-2.64903926849\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n",
      "torch.Size([50, 256, 4, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d7305b67945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# train with gradient penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mgradient_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_gradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4d7305b67945>\u001b[0m in \u001b[0;36mcalc_gradient_penalty\u001b[0;34m(netD, real_data, fake_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n\u001b[0;32m--> 145\u001b[0;31m                               grad_outputs=torch.ones(disc_interpolates.size()).cuda(gpu) if use_cuda else torch.ones(\n\u001b[0m\u001b[1;32m    146\u001b[0m                                   disc_interpolates.size()),\n\u001b[1;32m    147\u001b[0m                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
      "\u001b[0;32m/home/m.nakhodnov/anaconda3/envs/py2.7.14/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.save_images\n",
    "import tflib.mnist\n",
    "import tflib.plot\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    gpu = 0\n",
    "\n",
    "DIM = 64 # Model dimensionality\n",
    "BATCH_SIZE = 50 # Batch size\n",
    "CRITIC_ITERS = 5 # For WGAN and WGAN-GP, number of critic iters per gen iter\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "ITERS = 200000 # How many generator iterations to train for\n",
    "OUTPUT_DIM = 784 # Number of pixels in MNIST (28*28)\n",
    "\n",
    "lib.print_model_settings(locals().copy())\n",
    "\n",
    "# ==================Definition Start======================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        preprocess = nn.Sequential(\n",
    "            nn.Linear(128, 4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4*DIM, 2*DIM, 5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*DIM, DIM, 5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        deconv_out = nn.ConvTranspose2d(DIM, 1, 8, stride=2)\n",
    "\n",
    "        self.block1 = block1\n",
    "        self.block2 = block2\n",
    "        self.deconv_out = deconv_out\n",
    "        self.preprocess = preprocess\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.preprocess(input)\n",
    "        output = output.view(-1, 4*DIM, 4, 4)\n",
    "        #print output.size()\n",
    "        output = self.block1(output)\n",
    "        #print output.size()\n",
    "        output = output[:, :, :7, :7]\n",
    "        #print output.size()\n",
    "        output = self.block2(output)\n",
    "        #print output.size()\n",
    "        output = self.deconv_out(output)\n",
    "        output = self.sigmoid(output)\n",
    "        #print output.size()\n",
    "        return output.view(-1, OUTPUT_DIM)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.Conv2d(1, DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(OUTPUT_DIM, 4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(DIM, 2*DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2*DIM, 4*DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            # nn.LeakyReLU(True),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            # nn.LeakyReLU(True),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.output = nn.Linear(4*4*4*DIM, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 1, 28, 28)\n",
    "        out = self.main(input)\n",
    "        out = out.view(-1, 4*4*4*DIM)\n",
    "        out = self.output(out)\n",
    "        return out.view(-1)\n",
    "\n",
    "def generate_image(frame, netG):\n",
    "    noise = torch.randn(BATCH_SIZE, 128)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda(gpu)\n",
    "    noisev = autograd.Variable(noise, volatile=True)\n",
    "    samples = netG(noisev)\n",
    "    samples = samples.view(BATCH_SIZE, 28, 28)\n",
    "    # print samples.size()\n",
    "\n",
    "    samples = samples.cpu().data.numpy()\n",
    "\n",
    "    lib.save_images.save_images(\n",
    "        samples,\n",
    "        'tmp/mnist/samples_{}.png'.format(frame)\n",
    "    )\n",
    "\n",
    "# Dataset iterator\n",
    "train_gen, dev_gen, test_gen = lib.mnist.load(BATCH_SIZE, BATCH_SIZE)\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        for images,targets in train_gen():\n",
    "            yield images\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda(gpu) if use_cuda else alpha\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda(gpu)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(gpu) if use_cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "# ==================Definition End======================\n",
    "\n",
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "print(netG)\n",
    "print(netD)\n",
    "\n",
    "if use_cuda:\n",
    "    netD = netD.cuda(gpu)\n",
    "    netG = netG.cuda(gpu)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "if use_cuda:\n",
    "    one = one.cuda(gpu)\n",
    "    mone = mone.cuda(gpu)\n",
    "\n",
    "data = inf_train_gen()\n",
    "\n",
    "for iteration in xrange(ITERS):\n",
    "    start_time = time.time()\n",
    "    ############################\n",
    "    # (1) Update D network\n",
    "    ###########################\n",
    "    for p in netD.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "\n",
    "    for iter_d in xrange(CRITIC_ITERS):\n",
    "        _data = data.next()\n",
    "        real_data = torch.Tensor(_data)\n",
    "        if use_cuda:\n",
    "            real_data = real_data.cuda(gpu)\n",
    "        real_data_v = autograd.Variable(real_data)\n",
    "\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # train with real\n",
    "        D_real = netD(real_data_v)\n",
    "        D_real = D_real.mean()\n",
    "        # print D_real\n",
    "        D_real.backward(mone)\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(BATCH_SIZE, 128)\n",
    "        if use_cuda:\n",
    "            noise = noise.cuda(gpu)\n",
    "        noisev = autograd.Variable(noise, volatile=True)  # totally freeze netG\n",
    "        fake = autograd.Variable(netG(noisev).data)\n",
    "        inputv = fake\n",
    "        D_fake = netD(inputv)\n",
    "        D_fake = D_fake.mean()\n",
    "        D_fake.backward(one)\n",
    "\n",
    "        # train with gradient penalty\n",
    "        gradient_penalty = calc_gradient_penalty(netD, real_data_v.data, fake.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        D_cost = D_fake - D_real + gradient_penalty\n",
    "        Wasserstein_D = D_real - D_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "    ############################\n",
    "    # (2) Update G network\n",
    "    ###########################\n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "    netG.zero_grad()\n",
    "\n",
    "    noise = torch.randn(BATCH_SIZE, 128)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda(gpu)\n",
    "    noisev = autograd.Variable(noise)\n",
    "    fake = netG(noisev)\n",
    "    G = netD(fake)\n",
    "    G = G.mean()\n",
    "    G.backward(mone)\n",
    "    G_cost = -G\n",
    "    optimizerG.step()\n",
    "\n",
    "    # Write logs and save samples\n",
    "    lib.plot.plot('tmp/mnist/time', time.time() - start_time)\n",
    "    lib.plot.plot('tmp/mnist/train disc cost', D_cost.cpu().data.numpy())\n",
    "    lib.plot.plot('tmp/mnist/train gen cost', G_cost.cpu().data.numpy())\n",
    "    lib.plot.plot('tmp/mnist/wasserstein distance', Wasserstein_D.cpu().data.numpy())\n",
    "\n",
    "    # Calculate dev loss and generate samples every 100 iters\n",
    "    if iteration % 100 == 99:\n",
    "        dev_disc_costs = []\n",
    "        for images,_ in dev_gen():\n",
    "            imgs = torch.Tensor(images)\n",
    "            if use_cuda:\n",
    "                imgs = imgs.cuda(gpu)\n",
    "            imgs_v = autograd.Variable(imgs, volatile=True)\n",
    "\n",
    "            D = netD(imgs_v)\n",
    "            _dev_disc_cost = -D.mean().cpu().data.numpy()\n",
    "            dev_disc_costs.append(_dev_disc_cost)\n",
    "        lib.plot.plot('tmp/mnist/dev disc cost', np.mean(dev_disc_costs))\n",
    "\n",
    "        generate_image(iteration, netG)\n",
    "\n",
    "    # Write logs every 100 iters\n",
    "    if (iteration < 5) or (iteration % 100 == 99):\n",
    "        lib.plot.flush()\n",
    "\n",
    "    lib.plot.tick()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
